{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb3cf3e-dcdc-4e03-8669-e68cd01f2848",
   "metadata": {},
   "source": [
    "# 基礎集計\n",
    "\n",
    "1. 形態素解析  \n",
    "1. 係り受け解析  \n",
    "1. N-gram解析  \n",
    "上記を行い、カテゴリ別に出現する単語の傾向把握と、全カテゴリとのリフト値から差分を確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350307f6-4269-49f5-8e39-747fd50d2d55",
   "metadata": {},
   "source": [
    "# 出力ディレクトリの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f03082f-5d4a-4e24-b479-651a993b3ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変更箇所\n",
    "input_dir = \"../data/preprocessing/\"\n",
    "output_dir = \"../data/basic_agg/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca5b040-dbc8-4ef7-9e67-dc60bfd2ff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "存在するディレクトリです\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.isdir(output_dir):\n",
    "    print(\"存在するディレクトリです\")\n",
    "else:\n",
    "    os.makedirs(output_dir)\n",
    "    print(\"出力ディレクトリを作成しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68e1e2f-9373-4a9a-851c-3b5e351b4f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データ数:  1\n",
      "['../data/preprocessing/all_category_df.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "input_paths = glob.glob(f\"{input_dir}*.csv\")\n",
    "print(\"データ数: \", len(input_paths))\n",
    "print(input_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd389364-1449-4cf2-8a22-88b8f8265285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基礎集計に使用するDataFrameのパスを指定\n",
    "input_path = input_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d453ce8-1924-408e-ae80-49089eca1f5d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b88133bb-6464-4dfa-9c68-97dbc98c2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "\n",
    "mc = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16ae5a1-62b7-49d0-b5e0-79362864d865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dokujo-tsushin</td>\n",
       "      <td>友人代表のスピーチ、独女はどうこなしている?\\n もうすぐジューン・ブライドと呼ばれる0月。...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dokujo-tsushin</td>\n",
       "      <td>ネットで断ち切れない元カレとの縁\\n 携帯電話が普及する以前、恋人への連絡ツールは一般電話が...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                               text\n",
       "0  dokujo-tsushin  友人代表のスピーチ、独女はどうこなしている?\\n もうすぐジューン・ブライドと呼ばれる0月。...\n",
       "1  dokujo-tsushin  ネットで断ち切れない元カレとの縁\\n 携帯電話が普及する以前、恋人への連絡ツールは一般電話が..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_category_df = pd.read_csv(input_path)\n",
    "all_category_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4134c8-c724-4992-a46b-31455407dcd1",
   "metadata": {},
   "source": [
    "# 形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2deb9244-347a-4960-820c-d6e4079767a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun(text):\n",
    "    \"\"\" テキストから名詞を抜き出してカウンターを作成\"\"\"\n",
    "    \n",
    "    counter = {} # (名詞, 出現数)のcounter\n",
    "\n",
    "    for one_pos in mc.parse(text).split(\"\\n\"):\n",
    "        if (one_pos == \"EOS\") or (len(one_pos) == 0):\n",
    "            continue\n",
    "        surf, mc_result = one_pos.split(\"\\t\")\n",
    "        #品詞, 品詞細分類1,_,_,_,_,原型, _ \n",
    "        pos0, pos1, _, _, _, _, base, *_ = mc_result.split(\",\")\n",
    "        if (pos0 == \"名詞\") & (pos1 == \"一般\"):\n",
    "\n",
    "            if base == \"*\":\n",
    "                noun = surf\n",
    "            else:\n",
    "                noun = base\n",
    "\n",
    "            if noun in counter:\n",
    "                counter[noun] += 1\n",
    "            else:\n",
    "                counter[noun] = 1\n",
    "    \n",
    "    counter = sorted(counter.items(), \n",
    "                          key=lambda x:x[1], \n",
    "                          reverse=True)\n",
    "    return counter\n",
    "\n",
    "def extract_verb(text):\n",
    "    \"\"\" テキストから動詞を抜き出してカウンターを作成\"\"\"\n",
    "    \n",
    "    # Dockerfileで未知語の更新ができなかったため無理やり\n",
    "    # 記号系が後述の[名詞　サ変接続]に引っかかることを回避\n",
    "    mark = \"[!#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`\"\\\n",
    "           \"\\{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％ 　]\"\n",
    "    stop_mark = re.compile(mark)\n",
    "    word = \"(する|いる|れる|なる|ある|できる|られる|せる|おる|てる)\"\n",
    "    stop_word = re.compile(word)\n",
    "    \n",
    "    counter = {} # (名詞, 出現数)のcounter\n",
    "    for one_pos in mc.parse(text).split(\"\\n\"):\n",
    "        if (one_pos == \"EOS\") or (len(one_pos) == 0):\n",
    "            continue\n",
    "\n",
    "        surf, mc_result = one_pos.split(\"\\t\")\n",
    "        #品詞, 品詞細分類1,_,_,_,_,原型, _ \n",
    "        pos0, pos1, _, _, _, _, base, *_ = mc_result.split(\",\")\n",
    "\n",
    "        if (stop_mark.search(surf)) or (stop_word.search(base)):\n",
    "            continue\n",
    "    \n",
    "        if pos0 == \"動詞\":\n",
    "            if base == \"*\":\n",
    "                verb = surf\n",
    "            else:\n",
    "                verb = base\n",
    "            if verb in counter:\n",
    "\n",
    "                counter[verb] += 1\n",
    "            else:\n",
    "                counter[verb] = 1\n",
    "\n",
    "        elif (pos0 == \"名詞\") & (pos1 == \"サ変接続\"):\n",
    "                \n",
    "            if base == \"*\":\n",
    "                verb = f\"{surf}する\"\n",
    "            else:\n",
    "                verb = f\"{base}する\"\n",
    "\n",
    "            if verb in counter:\n",
    "                counter[verb] += 1\n",
    "            else:\n",
    "                counter[verb] = 1\n",
    "    \n",
    "    counter = sorted(counter.items(), \n",
    "                          key=lambda x:x[1], \n",
    "                          reverse=True)\n",
    "    return counter\n",
    "\n",
    "def extract_adjective(text):\n",
    "    \"\"\" テキストから名詞を抜き出してカウンターを作成\"\"\"\n",
    "    \n",
    "    counter = {} # (名詞, 出現数)のcounter\n",
    "\n",
    "    for one_pos in mc.parse(text).split(\"\\n\"):\n",
    "        if (one_pos == \"EOS\") or (len(one_pos) == 0):\n",
    "            continue\n",
    "        surf, mc_result = one_pos.split(\"\\t\")\n",
    "        #品詞, 品詞細分類1,_,_,_,_,原型, _ \n",
    "        pos0, pos1, _, _, _, _, base, *_ = mc_result.split(\",\")\n",
    "        if ((pos0 == \"形容詞\") & (pos1 == \"自立\")) or \\\n",
    "           ((pos0 == \"名詞\") & (pos1 == \"形容動詞語幹\")):\n",
    "\n",
    "            if base == \"*\":\n",
    "                noun = surf\n",
    "            else:\n",
    "                noun = base\n",
    "\n",
    "            if noun in counter:\n",
    "                counter[noun] += 1\n",
    "            else:\n",
    "                counter[noun] = 1\n",
    "    \n",
    "    counter = sorted(counter.items(), \n",
    "                          key=lambda x:x[1], \n",
    "                          reverse=True)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94d7530f-637c-481a-8c76-5916fad672f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_counter_df(texts, pos=\"\"):\n",
    "    \"\"\" 品詞別のカウントDataFrame作成\n",
    "    \n",
    "        args:\n",
    "            texts(list): データフレームの1カラムをリスト化\n",
    "            pos(str): noun or verb or adjective\n",
    "    \"\"\"\n",
    "    \n",
    "    counter = {}\n",
    "    for text in tqdm(texts):\n",
    "\n",
    "        if pos == \"noun\":\n",
    "            counter_1text = extract_noun(text)\n",
    "        elif pos == \"verb\":\n",
    "            counter_1text = extract_verb(text)\n",
    "        elif pos == \"adjective\":\n",
    "            counter_1text = extract_adjective(text)\n",
    "        else:\n",
    "            print(\"posを指定してください。空DFを返します\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        for key, val in counter_1text:\n",
    "            if key in counter:\n",
    "                counter[key] += 1\n",
    "            else:\n",
    "                counter[key] = 1\n",
    "\n",
    "    counter = sorted(counter.items(), \n",
    "                          key=lambda x:x[1], \n",
    "                          reverse=True)\n",
    "\n",
    "    result = pd.DataFrame(counter, columns=[pos, \"count\"])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a191a83-cf5c-4650-a972-85da04248115",
   "metadata": {},
   "source": [
    "## 全体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b99411f-b6aa-4444-951a-fac36380ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7367/7367 [00:15<00:00, 470.09it/s]\n",
      "100%|██████████| 7367/7367 [00:16<00:00, 446.51it/s]\n",
      "100%|██████████| 7367/7367 [00:14<00:00, 519.84it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = all_category_df[\"text\"].tolist()\n",
    "\n",
    "for pos in [\"noun\", \"verb\", \"adjective\"]:\n",
    "    result = create_pos_counter_df(texts, pos=pos)\n",
    "    \n",
    "    # カテゴリ別リフト値を算出するために使用\n",
    "    count_sum = result[\"count\"].sum()\n",
    "    result[\"ratio\"] = result[\"count\"].apply(lambda x: x / count_sum)\n",
    "\n",
    "    result.to_csv(f\"{output_dir}all_{pos}.csv\", \n",
    "                  encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742a6449-b207-494e-adcf-d41bd84a1f1d",
   "metadata": {},
   "source": [
    "## カテゴリ別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a6db02f-8467-4c89-9a15-ada576feb747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 870/870 [00:02<00:00, 384.97it/s]\n",
      "100%|██████████| 870/870 [00:01<00:00, 483.31it/s]\n",
      "100%|██████████| 864/864 [00:01<00:00, 712.45it/s]\n",
      "100%|██████████| 511/511 [00:01<00:00, 366.96it/s]\n",
      "100%|██████████| 870/870 [00:02<00:00, 394.36it/s]\n",
      "100%|██████████| 842/842 [00:01<00:00, 449.56it/s]\n",
      "100%|██████████| 870/870 [00:02<00:00, 400.69it/s]\n",
      "100%|██████████| 900/900 [00:01<00:00, 847.60it/s]\n",
      "100%|██████████| 770/770 [00:00<00:00, 825.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 870/870 [00:02<00:00, 331.38it/s]\n",
      "100%|██████████| 870/870 [00:02<00:00, 431.31it/s]\n",
      "100%|██████████| 864/864 [00:01<00:00, 660.87it/s]\n",
      "100%|██████████| 511/511 [00:01<00:00, 345.73it/s]\n",
      "100%|██████████| 870/870 [00:02<00:00, 375.46it/s]\n",
      "100%|██████████| 842/842 [00:02<00:00, 404.75it/s]\n",
      "100%|██████████| 870/870 [00:02<00:00, 363.48it/s]\n",
      "100%|██████████| 900/900 [00:01<00:00, 712.52it/s]\n",
      "100%|██████████| 770/770 [00:01<00:00, 719.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjective\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 870/870 [00:02<00:00, 384.71it/s]\n",
      "100%|██████████| 870/870 [00:01<00:00, 495.64it/s]\n",
      "100%|██████████| 864/864 [00:01<00:00, 757.87it/s]\n",
      "100%|██████████| 511/511 [00:01<00:00, 389.70it/s]\n",
      "100%|██████████| 870/870 [00:02<00:00, 417.24it/s]\n",
      "100%|██████████| 842/842 [00:01<00:00, 464.34it/s]\n",
      "100%|██████████| 870/870 [00:02<00:00, 412.39it/s]\n",
      "100%|██████████| 900/900 [00:01<00:00, 788.88it/s]\n",
      "100%|██████████| 770/770 [00:00<00:00, 843.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for pos in [\"noun\", \"verb\", \"adjective\"]:\n",
    "    print(pos)\n",
    "    all_df = pd.read_csv(f\"{output_dir}all_{pos}.csv\")\n",
    "    all_df = all_df.rename(columns={\"count\": \"all_count\",\n",
    "                                    \"ratio\": \"all_ratio\"})\n",
    "\n",
    "    for category in all_category_df[\"category\"].unique():\n",
    "        texts = all_category_df.query(\"category == @category\")[\"text\"].tolist()\n",
    "\n",
    "        result = create_pos_counter_df(texts, pos=pos)\n",
    "        # カテゴリ別リフト値を算出するために使用\n",
    "        count_sum = result[\"count\"].sum()\n",
    "        result[\"ratio\"] = result[\"count\"].apply(lambda x: x / count_sum)\n",
    "        \n",
    "        result = pd.merge(result, all_df, on=pos, how=\"left\")\n",
    "        result[\"lift\"] = result[\"ratio\"] / result[\"all_ratio\"]\n",
    "        \n",
    "        result.to_csv(f\"{output_dir}{category}_{pos}.csv\", \n",
    "                      encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f60a0-7947-4616-8c67-11396101f6e4",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9579bc3-66ca-40ec-8523-da5df5520ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(text_list, n):\n",
    "    return [text_list[idx:idx+n] for idx in range(len(text_list))]\n",
    "\n",
    "def create_n_gram_counter_df(texts, n):\n",
    "    \"\"\" 品詞別のカウントDataFrame作成\n",
    "    \n",
    "        args:\n",
    "            texts(list): データフレームの1カラムをリスト化\n",
    "            n(int): n-gramの数\n",
    "        Note:\n",
    "            バグ：前単語と後単語のn-gramが入っている\n",
    "                例　[\"スマート\", \"フォン\", \"関連する\", \"記事\"]の場合\n",
    "                    理想： \"スマート-フォン\", \"関連する-記事\"\n",
    "                    現実: \"スマート-フォン\", \"フォン-関連する\", \"関連する-記事\"\n",
    "    \"\"\"\n",
    "    # Dockerfileで未知語の更新ができなかったため無理やり\n",
    "    # 記号系が後述の[名詞　サ変接続]に引っかかることを回避\n",
    "    mark = \"[!#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`\"\\\n",
    "           \"\\{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％ 　]\"\n",
    "    stop_mark = re.compile(mark)\n",
    "    word = \"(する|いる|れる|なる|ある|できる|られる|せる|おる|てる)\"\n",
    "    stop_word = re.compile(word)\n",
    "    \n",
    "    counter  = {}\n",
    "    for text in tqdm(texts):\n",
    "        \n",
    "        surf_list = []\n",
    "        past_i = 0\n",
    "        past_pos = \"\"\n",
    "        for i, one_pos in enumerate(mc.parse(text).split(\"\\n\")):\n",
    "            if (one_pos == \"EOS\") or (len(one_pos) == 0):\n",
    "                continue\n",
    "\n",
    "            # n-gramに使用する品詞のみのリスト作成\n",
    "            surf, mc_result = one_pos.split(\"\\t\")\n",
    "            #品詞, 品詞細分類1,_,_,_,_,原型, _ \n",
    "            pos0, pos1, _, _, _, _, base, *_ = mc_result.split(\",\")\n",
    "            if (stop_mark.search(surf)) or (stop_word.search(base)):\n",
    "                continue\n",
    "            \n",
    "            if (pos0 in [\"名詞\", \"動詞\", \"形容詞\"]) & \\\n",
    "               (pos1 in [\"一般\", \"固有名詞\", \"自立\",\n",
    "                         \"サ変接続\", \"形容動詞語幹\"]):\n",
    "\n",
    "                if (pos0 == \"名詞\") & (pos1 == \"サ変接続\"):\n",
    "                    if base == \"*\":\n",
    "                        pos = f\"{surf}する\"\n",
    "                    else:\n",
    "                        pos = f\"{base}する\"\n",
    "                else:\n",
    "                    if base == \"*\":\n",
    "                        pos = surf\n",
    "                    else:\n",
    "                        pos = base\n",
    "                \n",
    "                if (i - past_i) == 1:\n",
    "                    if surf_list:\n",
    "                        if surf_list[-1] != past_pos:\n",
    "                            surf_list.append(past_pos)\n",
    "                    surf_list.append(pos)\n",
    "                    \n",
    "                past_i = i\n",
    "                past_pos = pos\n",
    "\n",
    "            else:\n",
    "                past_i = 0\n",
    "                past_pos = \"\"\n",
    "\n",
    "        # n-gramによるカウンター作成\n",
    "        n_gram_list = n_gram(surf_list, n)\n",
    "        for one_n_gram in n_gram_list:\n",
    "            one_n_gram_str = \"-\".join(one_n_gram)\n",
    "\n",
    "            if one_n_gram_str in counter:\n",
    "                counter[one_n_gram_str] += 1\n",
    "            else:\n",
    "                counter[one_n_gram_str] = 1\n",
    "\n",
    "    counter = sorted(counter.items(), \n",
    "                     key=lambda x:x[1], \n",
    "                     reverse=True)\n",
    "    result = pd.DataFrame(counter, columns=[f\"{n}gram\", \"count\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca4afaf5-8977-44d9-a606-60708e24a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7367/7367 [00:18<00:00, 408.54it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = all_category_df[\"text\"].tolist()\n",
    "n = 2\n",
    "result = create_n_gram_counter_df(texts, n=n)\n",
    "\n",
    "# カテゴリ別リフト値を算出するために使用\n",
    "count_sum = result[\"count\"].sum()\n",
    "result[\"ratio\"] = result[\"count\"].apply(lambda x: x / count_sum)\n",
    "\n",
    "result.to_csv(f\"{output_dir}all_{n}gram.csv\", \n",
    "              encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8cd3db1-72ee-4d50-93dc-c112075787a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 870/870 [00:02<00:00, 329.74it/s]\n",
      "100%|██████████| 870/870 [00:02<00:00, 425.16it/s]\n",
      "100%|██████████| 864/864 [00:01<00:00, 564.20it/s]\n",
      "100%|██████████| 511/511 [00:01<00:00, 298.06it/s]\n",
      "100%|██████████| 870/870 [00:02<00:00, 366.78it/s]\n",
      "100%|██████████| 842/842 [00:02<00:00, 404.63it/s]\n",
      "100%|██████████| 870/870 [00:02<00:00, 299.28it/s]\n",
      "100%|██████████| 900/900 [00:01<00:00, 726.57it/s]\n",
      "100%|██████████| 770/770 [00:01<00:00, 732.61it/s]\n"
     ]
    }
   ],
   "source": [
    "for category in all_category_df[\"category\"].unique():\n",
    "\n",
    "    texts = all_category_df.query(\"category == @category\")[\"text\"].tolist()\n",
    "    n = 2\n",
    "    result = create_n_gram_counter_df(texts, n=n)\n",
    "\n",
    "    # カテゴリ別リフト値を算出するために使用\n",
    "    count_sum = result[\"count\"].sum()\n",
    "    result[\"ratio\"] = result[\"count\"].apply(lambda x: x / count_sum)\n",
    "\n",
    "    result.to_csv(f\"{output_dir}{category}_{n}gram.csv\", \n",
    "                  encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b16a3-7e79-4d1c-b998-4671389e5195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b4915-052c-4839-a352-75b9c2e3c7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
